Glossary

Sigmoid:
  sigmoid(x) = 1/(1+e^x)
  Sigmoid bounds the output x between 0 and 1 and is useful for probabilities and other
  values that need to be bounded between 0 and 1
Linear regression:
  Model relationship between one or more independent variables and a dependent scalar
  If only one input variable, then simple linear regression, else multiple linear regression
  Simple linear regression is useful for finding relationship between two continuous variables
  Multivariate linear regression is where multiple coorelated dependent variables are predicted
Logistic regression:
  Models the probability of certain classes or events existing. Binary is when there are only two classes
  For more than two, each class is assigned a probability score and the sum of probabilities adds to 1
  Sigmoid is usually used for such functions as that stays between 0 and 1
  yhat = sigmoid (wt*x + b)
Loss function:
  Loss function is a function of difference between expected and actual prediction. Loss (yhat, y)
  MSE doesnt work well with gradient descent for Logistic regression
  So use L(yhat, y) = -(y.log(yhat) + (1-y).log(1-yhat))
  If y=1, L = -log(yhat), so yhat should be as large as possible, close to 1 since sigmoid taken
  If y=0, L = log(1-yhat), so yhat should be as close to 0 as possible
Cost function:
  Usually average of the above loss functions + regularization etc.
Cross-entropy loss:
  Log loss between 0 and 1. When the true prediction is 1 and the actual prediction
  is lower, as the actual prediction gets closer to 0, the loss increases exponentially
def CrossEntropy(yHat, y):
    if y == 1: # if true prediction is positive
      return -log(yHat) # decrease loss the closer the prediction is to 1
    else:
      return -log(1 - yHat) # increase loss if prediction goes to 1
Object detection:
  Finding objects of interest in an image
Bounding boxes:
  Boxes drawn around objects of interest
  Each bounding box also has a classification probability associated
  that predicts the likelyhood of classes
Confidence score:
  Bounding boxes also have a confidence score associated that says
  how likely this box contains an (any) object
NMS:
  Boxes whose confidence score is low (below a certain threshold) can
  be filtered out to avoid too many bounding boxes to process. This is
  called Non-Max Supression (NMS)
Multistage detector:
  First stage, a region proposal is created of regions that may contain objects
  Second stage, prediction done for these regions. E.g. Faster R-CNN
Onestage detector:
  Single pass and predicts all bounding boxes in one go. E.g. Yolo, SSD
Bias/Variance
  High Bias when we underfit the data. Here both the training and test errors are high
  To fix high bias, try larger network, train longer or different NN architecture
  High Variance when we overfit the data. Large diff between training and test accuracy
  To fix high variance, get more data, regularization or different NN architecture
  High bias (avoidable bias) if big diff between train and expected accuracy
  Mismatch between distributions if low diff between train and train-dev but high diff with dev
  High variance if big diff between train and train-dev datasets
Regularization
  Used to reduce the variance
  Add lambda (regularization hyperparameter) * regularization function to the cost function
  L2 regularization is also called weight decay. The derivative is lambda/m*W[l]
  Regularization function could be euclidean norm (L2 regularization)
  Frobenius norm: sqrt(sum of square of elements)
  With L1 regularization, lot of w values become zero (helps sparsity)
  Regularization prevents overfitting by linearizing the layers (e.g. keeping w low keeps activation
  in the linear range of tanh). Thus the whole network gets closer to linear than non-linear
  preventing complex functions preventing overfitting
  Dropout is another form of regularizaton
  Implemented by multiplying activation with matrix of zeros and ones with probability of dropout value
  and then dividing all remaning elements by dropout value to make sure that activation sum is ~ same
  Dropout prevents the weights from relying on one feature alone as that feature
  may disappear due to dropout
  Other regularization techniques include data augmentation, early stopping (stop when test set
  error starts increasing to prevent overfitting)
Normalizing training set
  x-u (mean) and then divide by sigma (normalizes the variance)
  Normalizing makes the cost function simpler and easier to optimize. Plotting the Cost wrt
  weight and bias will be a nice convex function after normalizing, while without normalizing
  it would be very sharp and so the learning rate might need to be small
Differentiable function:
  Can find the slope of the  curve at any two points (not possible when doing quantization). This enables backpropagation
Activation functions:
  Linear activation functions f(x) = x
  Need non-linear activation functions as if you use only linear activation functions, the composition
  of two or more linear activation functions is equivalent to a linear activation function, so no
  use of using multiple layers. Usually used only in last layer
  Sigmoid (aka Logistic Activation Function): Between [0, 1]
  Sigmoid is useful almost only as the last activation function of a binary classifier as the
  outputs we need are to be between 0 and 1
  Tanh: [-1, 1] = (1-e^-2x)/(1+e^-2x) == (e^x - e^-x)/(e^x + e^-x)  
  Tanh is better than sigmoid as it is centered on 0 instead of 0.5 for Sigmoid
  Both tanh and sigmoid have the issue that the slope is very small as the values get very large
  or small
Gradient descent:
  When entire training set is passed through before the backward pass, its batch gradient descent
  When only a subset of the batch is passed through forward pass before bprop, its minibatch gradient descent
  When fprop and bprop done on each example, its stochastic gradient descent
  With batch gradient descent, the loss curve is smooth but it takes too long per iteration and is memory expensive
  With minibatch gradient descent it is noisy and may go in the  wrong direction a few iterations, but is faster
  Stochastic gradient descent is very noisy but is slow due to non-vectorization
Momentum:
  Momentum dampens the gradient descent from oscilating too much by implementing an exponential moving average
  on the dW and dB. This averages out the conflicting directions that each minibatch goes in, while
  adding the direction when they are the same. The hyperparameter is beta. Value of 0.9 means averaging over
  last 10 values with Vdw = beta*Vdw + (1-beta)dw
  Exponential weighted average where beta = 0.9 = V(10) = 0.9*V(9) + 0.1Theta(10)
  V(10) = 0.1Theeta(10) + 0.9*0.1T(9) + 0.9^2*0.1T(8) + 0.9^3*0.1T(7)...
  Might need to do beta correction (which is needed when initial value of V is 0 and that affects intial values)
  Two ways is, either use warmup or do V=V/(1-beta^t) where t is the iteration
  Beta is usually 0.9
RMSProp:
  Sdw = beta*Sdw + (1-beta)dw^2
  W = W - LR*dw/sqrt(Sdw + 10e-7 (epsilon))
  Same with Bias
  Intiution is that when the slope is much larger, Sdx will be much larger so dividing by sqrt(Sdx) slows it down
  Beta is usually 0.999
Adam is RMSProp + Momentum (Adaptive Momentum estimation)
Learning Rate decay:
  Nearer convergence, take slower steps so that it oscilates in a tighter region around the minimum
  lr = lr/(1+decay*epoch_num)
Hyperparameter tuning:
  LR (alpha), Beta, Beta1/2 minibatch, momemntum, number of hidden layers etc are hyperpamaters
  Rather than trying a grid it is better to try random values (that way we get higher range of both axis)
  in case of trying out two hyperpamaters
  Also scale is important. When considing random values between 0.0001 and 0.1, choosing a random
  range is not helpful as you need equal points between 0.0001 and 0.001 as you have between 0.01 and 0.1
Batch normalization:
  Normalizes the activations with mean and variance. This speeds up the training because normalizing the inputs
  to each layer enables the model to train on shifted data (covariant shifting). Same shift happens in the hidden
  units during training and so BN avoids these shifts improving stablity. BN also actsa s a regularizer
  by adding noise (as the bias and variance varies from batch to batch)
  BN is usually removed during inference
  Usually done on Z (input to activations) instead of A (activations).
  Controlled by two learnable parameters beta and gamma
  Z(i)norm = (Z(i) - u)/sqrt(sigma^2 + epsilon) where u is Mu = 1/m*np.sum(Z(i)) and sqrt(sigma^2) is variance and
  sigma^2 = 1/m*np.sum(Z(i) - u)^2
  Zhat(i) = beta*Z(i)norm + gamma
  Beta maps to the variance and gamma to the mean. As in, if Beta = Variance and gamma = mean, then it is
  equivalent to no BN being applied
Softmax:
  Used as activation on last layer of multiclass classification problem. Gives probabilities that add to one
  t = e^Z; A = t/np.sum(t)
  Different from hard max where only one value has output 1 and everything else is 0
Precision:
  Of all the images predicted, how many (what percent) are correction
  Precision = TP / (TP + FP)
Recall:
  Of all the correct images, how many (what percent) were predicted
  Recall = TP / (TP + FN)
F1 score:
  Harmonic mean of precision and recall. 2/(1/Precision + 1/Recall)
IOU (Intersection over Union) aka Jaccard index:
  Area of overlap / Area of union
Bayes optimial error:
  Best possible error (not possible to go beyond this accuracy)
Transfer learning:
  Replace last (or last few) layers of a neural network trained on some data
  with uninitialized weights/biases and then train for a different task
  Usually useful when the tasks are similar and the amount of data we have for
  the new task is less
Multitask learning:
  Learn multiple features at the same time. Performs better than having different NNs
  Multitask learning useful when training on set of tasks that share low level features
  Amount of data for each task is similar


Neural networks
Moving from sigmoid to Relu helps gradient descent train faster because
sigmoid had really flat slopes at the ends that made training really slow
Consider Logical regression
Forward pass:
  z = w1*x1 + w2*x2 + b
  yhat = Sigmoid(z) (use yhat == a for simplicity)
  Loss = -(y*log(yHat) + (1-y)*log(1-yHat))
  Cost is average of all losses. So cost is 1/m * sum(Loss(yhat, y))
Doing derivatives for the backward pass:
  dL(yhat, y)/dyhat 
  since a == yhat, this becomes dL(a, y)/da. This can be shortened as da
  So da = dL(a, y)/da = -y*log(a)/da -(1-y)*log(1-a)/da
  = -y/a + (1-y)/(1-a)
  Now calculate the derivative wrt z; dL(a, y)/dz = dL/dz = dL/da * da/dz (per chain rule)
  da/dz = d(1/(1+e^a))/dz = a * (1-a)
  dL/dz = dl/da * da/dz = a * (1-a) * (-y/a + (1-y)/(1-a)) = a - y
  dw1 = dL/dw1 = dL/dz * dz/dw1 = (a - y) * x1
  dw2 = (a - y) * x2
  db = a - y
  Weight updte: w1 = w1 - lr*dw1; w2 = w2 - lr*dw2; b = b - lrdb
Neural network can just be considered as a number of logistic regression steps put together

Forward pass:
  Z1 = W1*A0 + B1
  A1 = G(Z1)

Backward pass:
  dzl = da*g'(z)
  dwl = dzl*a(l-1)
  dbl = dzl
  da[l-1] = dzl*wl



def forward_propagation_n(X, Y, parameters):
    """
    Implements the forward propagation (and computes the cost) presented in Figure 3.
    
    Arguments:
    X -- training set for m examples
    Y -- labels for m examples 
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
                    W1 -- weight matrix of shape (5, 4)
                    b1 -- bias vector of shape (5, 1)
                    W2 -- weight matrix of shape (3, 5)
                    b2 -- bias vector of shape (3, 1)
                    W3 -- weight matrix of shape (1, 3)
                    b3 -- bias vector of shape (1, 1)
    
    Returns:
    cost -- the cost function (logistic cost for one example)
    """
    
    # retrieve parameters
    m = X.shape[1]
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]

    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    # Cost
    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)
    cost = 1./m * np.sum(logprobs)
    
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
    
    return cost, cache

def backward_propagation_n(X, Y, cache):
    """
    Implement the backward propagation presented in figure 2.
    
    Arguments:
    X -- input datapoint, of shape (input size, 1)
    Y -- true "label"
    cache -- cache output from forward_propagation_n()
    
    Returns:
    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.
    """
    
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T) * 2
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,
                 "dA2": dA2, "dZ2": dZ2, "dW2": dW2, "db2": db2,
                 "dA1": dA1, "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients


Convolution:
  Pass filter (aka kernel) through image and do sum of (elementwise multiply of each)
  a11*f11 + a12*f12... + a32*f32 + a33*f33  <-- gives one element of output
  Filters can be considered as feature detectors, say edge detectors or circle detectors
  Each filter will thus morph into a different feature detector during training
  Padding so that corner and edge elements get more filters to pass through
  Stride to downsample further
  Convolution represented using * so A*F
  nIxnI * nFxnF = nOxnO. Size of nO = nI - nF + 1
  With Padding and Strides
  nO = (nI + 2P - nF)/S  + 1
  Number of outputs (or channels in a single output) equal to number of filters
  In 3D convolutions, filters also have same number of channels as the input, but
  output has only one channel as the entire 3D convolve is summed.
  Bias is applied to each entire output's channel. Thus number of bias = num_channels
  In convolution, size of the parameters depends much lesser on size of input (compared
  to FC), hence it scales much better for larger images
  The reason that convs have lot lesser parameters is that conv filters do parameter sharing
  So, the same edge detector can be used throughout the image.
  Another reason for less parameters is that there is sparsity of connection. Each output
  element only depends on a subset of the inputs (unlike FC)


General training tips
Ways to improve accuracy of the model include
  Get more data
  Get diverse training set
  Train longer
  Try different Optimizer: Adam instead of GradientDescent
  Try a bigger or smaller model
  Try a different network architecture (Activation functions, hidden units, wider network)
  Try dropouts, L2 regularization,
Make sure that dev/test sets come from the same distribution (training set having different
distribution is a bit ok). To check what the bias and variance is on this, take a subset of
the training set and call it the training-dev set and then compare the accuracy against both
the training-dev set and the dev set.

Object Detection:

Classifiers do not work well when there are multiple objects in the image
Object detectors predicts bounding boxes for each image and so can better predict the image
Bounding boxes require four additional numbers for each of the corners
Object detectors have two outputs
1. The usual classification prediction
2. Bounding box regression
Thus the loss function is an addition of the regression loss for bounding box (usually MSE)
and cross-entropy loss for classification
Bounding boxes need to specialize to only handle one object. Otherwise they will end up taking
the average of all objects and fail. Hence each detector is assigned a specific position in image
For that, a grid is created and each detector focuses on only one cell in the grid
There can be multiple detectors for each grid, with each grid focusing on image of different sizes
For example, one detector might be for a square object bb, one for a long object and one for a wide object
A detector is responsible for detecting an image only if the center of the image lies in the grid
that the detector owns. This prevents multiple detectors of different grids competing to detect one object
Yolov3 has three grids of sizes 13x13, 19x19
Yolov3 detector outputs 85 numbers, 80 for the class probabilities, 4 for bounding box coordinates and
one for confidence score (that an object exists for that bounding box)
If there are 5 detectors, there would be 5*85 channels = 425 channels
If a grid ix 13x13, and 5 detectors, then there would be 13x13x5=845 predictions
To avoid too many predictions, the confidence score (object likely?) is used to remove most predictions
NMS removes all boxes that overlap (beyond a certain threshold called nms threshold) with the box
that has the highest confidence
At the end of these two, there would be around 10 predictions out of possible 845
Anchors refer to the 5 box sizes (square, long wide etc.) which map to each detector
Thus there will be as many anchors as detectors and detectors map 1:1 to anchors


Yolov3:
Coco has 80 classes
Yolov3 has three detectors/anchorboxes
Grids are called as cells in the paper. Similarly anchors == bounding boxes
Predicts bounding boxes, class probabilities and objectness (likelyhood that there is an object in the box)
Bounding boxes:
The network predicts 4 coordinates, tx, ty, tw, th. 
If the cell (grid) offset is at cx, cy and the bounding box (anchor) priors (default shapes) are ph and pw, then
bx = sigma(tx) + cx
by = sigma(ty) + cy
bh = ph*e^th
bw = pw*e^tw
MSE used for calculating loss of coordinates t_expected - t_predicted
Objectness:
Logistic regression used. 1 if BB prior overlaps target more than any other BB
Class probabilities:
Multilabel classifier (independent logistic classifiers and not softmax as it didnt impact perf), hence BCELoss
instead of BCEWithLogitsLoss. Also using softmax assumes that each box can have only one class label which is not case
in open images dataset (woman + person)
Three boxes (input to three yolo layers) predicted. Size = N x N x three anchors x (80 class preds + 1 objectness + 4 BB offsets)
Feature map from two layers before first grid (13x13) taken and then upsampled by 2x and then merged with a feature map from
even earlier in the network. Gives more semantic info from upsampled feature map and finer grain from earlier feature map. Add a
few convolution to get a prediction twice the size. Repeat one more time for third Yolo layer input



