Glossary

Sigmoid:
  sigmoid(x) = 1/(1+e^x)
  Sigmoid bounds the output x between 0 and 1 and is useful for probabilities and other
  values that need to be bounded between 0 and 1
Linear regression:
  Model relationship between one or more independent variables and a dependent scalar
  If only one input variable, then simple linear regression, else multiple linear regression
  Simple linear regression is useful for finding relationship between two continuous variables
  Multivariate linear regression is where multiple coorelated dependent variables are predicted
Logistic regression:
  Models the probability of certain classes or events existing. Binary is when there are only two classes
  For more than two, each class is assigned a probability score and the sum of probabilities adds to 1
  Sigmoid is usually used for such functions as that stays between 0 and 1
  yhat = sigmoid (wt*x + b)
Loss function:
  Loss function is a function of difference between expected and actual prediction. Loss (yhat, y)
  MSE doesnt work well with gradient descent for Logistic regression
  So use L(yhat, y) = -(y.log(yhat) + (1-y).log(1-yhat))
  If y=1, L = -log(yhat), so yhat should be as large as possible, close to 1 since sigmoid taken
  If y=0, L = log(1-yhat), so yhat should be as close to 0 as possible
Cost function:
  Usually average of the above loss functions + regularization etc.
Cross-entropy loss:
  Log loss between 0 and 1. When the true prediction is 1 and the actual prediction
  is lower, as the actual prediction gets closer to 0, the loss increases exponentially
def CrossEntropy(yHat, y):
    if y == 1: # if true prediction is positive
      return -log(yHat) # decrease loss the closer the prediction is to 1
    else:
      return -log(1 - yHat) # increase loss if prediction goes to 1
Object detection:
  Finding objects of interest in an image
Bounding boxes:
  Boxes drawn around objects of interest
  Each bounding box also has a classification probability associated
  that predicts the likelyhood of classes
Confidence score:
  Bounding boxes also have a confidence score associated that says
  how likely this box contains an (any) object
NMS:
  Boxes whose confidence score is low (below a certain threshold) can
  be filtered out to avoid too many bounding boxes to process. This is
  called Non-Max Supression (NMS)
Multistage detector:
  First stage, a region proposal is created of regions that may contain objects
  Second stage, prediction done for these regions. E.g. Faster R-CNN
Onestage detector:
  Single pass and predicts all bounding boxes in one go. E.g. Yolo, SSD
IOU (Intersection over Union) aka Jaccard index:
  Area of overlap / Area of union
Bias/Variance
  High Bias when we underfit the data. Here both the training and test errors are high
  To fix high bias, try larger network, train longer or different NN architecture
  High Variance when we overfit the data. Large diff between training and test accuracy
  To fix high variance, get more data, regularization or different NN architecture
Regularization
  Used to reduce the variance
  Add lambda (regularization hyperparameter) * regularization function to the cost function
  L2 regularization is also called weight decay. The derivative is lambda/m*W[l]
  Regularization function could be euclidean norm (L2 regularization)
  Frobenius norm: sqrt(sum of square of elements)
  With L1 regularization, lot of w values become zero (helps sparsity)
  Regularization prevents overfitting by linearizing the layers (e.g. keeping w low keeps activation
  in the linear range of tanh). Thus the whole network gets closer to linear than non-linear
  preventing complex functions preventing overfitting
  Dropout is another form of regularizaton
  Implemented by multiplying activation with matrix of zeros and ones with probability of dropout value
  and then dividing all remaning elements by dropout value to make sure that activation sum is ~ same
  Dropout prevents the weights from relying on one feature alone as that feature
  may disappear due to dropout
  Other regularization techniques include data augmentation, early stopping (stop when test set
  error starts increasing to prevent overfitting)
Normalizing training set
  x-u (mean) and then divide by sigma (normalizes the variance)
  Normalizing makes the cost function simpler and easier to optimize. Plotting the Cost wrt
  weight and bias will be a nice convex function after normalizing, while without normalizing
  it would be very sharp and so the learning rate might need to be small


Neural networks
Moving from sigmoid to Relu helps gradient descent train faster because
sigmoid had really flat slopes at the ends that made training really slow
Consider Logical regression
Forward pass:
  z = w1*x1 + w2*x2 + b
  yhat = Sigmoid(z) (use yhat == a for simplicity)
  Loss = -(y*log(yHat) + (1-y)*log(1-yHat))
  Cost is average of all losses. So cost is 1/m * sum(Loss(yhat, y))
Doing derivatives for the backward pass:
  dL(yhat, y)/dyhat 
  since a == yhat, this becomes dL(a, y)/da. This can be shortened as da
  So da = dL(a, y)/da = -y*log(a)/da -(1-y)*log(1-a)/da
  = -y/a + (1-y)/(1-a)
  Now calculate the derivative wrt z; dL(a, y)/dz = dL/dz = dL/da * da/dz (per chain rule)
  da/dz = d(1/(1+e^a))/dz = a * (1-a)
  dL/dz = dl/da * da/dz = a * (1-a) * (-y/a + (1-y)/(1-a)) = a - y
  dw1 = dL/dw1 = dL/dz * dz/dw1 = (a - y) * x1
  dw2 = (a - y) * x2
  db = a - y
  Weight updte: w1 = w1 - lr*dw1; w2 = w2 - lr*dw2; b = b - lrdb
Neural network can just be considered as a number of logistic regression steps put together
Activation functions:
Linear activation functions f(x) = x
Need non-linear activation functions as if you use only linear activation functions, the composition
of two or more linear activation functions is equivalent to a linear activation function, so no
use of using multiple layers. Usually used only in last layer
Sigmoid: Between [0, 1]
Sigmoid is useful almost only as the last activation function of a binary classifier as the
outputs we need are to be between 0 and 1
Tanh: [-1, 1] = (1-e^-2x)/(1+e^-2x) == (e^x - e^-x)/(e^x + e^-x)  
Tanh is better than sigmoid as it is centered on 0 instead of 0.5 for Sigmoid
Both tanh and sigmoid have the issue that the slope is very small as the values get very large
or small




Object Detection:

Classifiers do not work well when there are multiple objects in the image
Object detectors predicts bounding boxes for each image and so can better predict the image
Bounding boxes require four additional numbers for each of the corners
Object detectors have two outputs
1. The usual classification prediction
2. Bounding box regression
Thus the loss function is an addition of the regression loss for bounding box (usually MSE)
and cross-entropy loss for classification
Bounding boxes need to specialize to only handle one object. Otherwise they will end up taking
the average of all objects and fail. Hence each detector is assigned a specific position in image
For that, a grid is created and each detector focuses on only one cell in the grid
There can be multiple detectors for each grid, with each grid focusing on image of different sizes
For example, one detector might be for a square object bb, one for a long object and one for a wide object
A detector is responsible for detecting an image only if the center of the image lies in the grid
that the detector owns. This prevents multiple detectors of different grids competing to detect one object
Yolov3 has three grids of sizes 13x13, 19x19
Yolov3 detector outputs 85 numbers, 80 for the class probabilities, 4 for bounding box coordinates and
one for confidence score (that an object exists for that bounding box)
If there are 5 detectors, there would be 5*85 channels = 425 channels
If a grid ix 13x13, and 5 detectors, then there would be 13x13x5=845 predictions
To avoid too many predictions, the confidence score (object likely?) is used to remove most predictions
NMS removes all boxes that overlap (beyond a certain threshold called nms threshold) with the box
that has the highest confidence
At the end of these two, there would be around 10 predictions out of possible 845
Anchors refer to the 5 box sizes (square, long wide etc.) which map to each detector
Thus there will be as many anchors as detectors and detectors map 1:1 to anchors



Z1 = W1*A0 + B1
A1 = G(Z1)

dzl = da*g'(z)
dwl = dzl*a(l-1)
dbl = dzl
da[l-1] = dzl*wl



